{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47b4fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from itemloaders.processors import MapCompose, TakeFirst\n",
    "from scrapy.loader import ItemLoader\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de47c615",
   "metadata": {},
   "source": [
    "---\n",
    "# This project is to scrape the 2024-2025 Champions league group stage table\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ed481b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-13 13:56:12 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-09-13 13:56:12 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Windows-10-10.0.22631-SP0\n",
      "2024-09-13 13:56:12 [py.warnings] WARNING: C:\\Users\\conso\\.conda\\envs\\py310\\lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ OUR RESPONSE ]\n"
     ]
    }
   ],
   "source": [
    "class ChampionsLeagueScrapeSpider(scrapy.Spider):\n",
    "    name = 'championspider'\n",
    "\n",
    "    # Website spider sends requests to\n",
    "    start_urls = ['https://www.espn.com/soccer/table/_/league/uefa.champions']\n",
    "    \n",
    "    # This will help to save the data generated in CSV format\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'champions_league_table.json': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        },\n",
    "        'LOG_LEVEL': 'WARNING',  # Set the logging level to WARNING\n",
    "        #'LOG_FILE': 'scrapy_log.txt'  # Optional: Save log messages to a file\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('[ OUR RESPONSE ]')\n",
    "        \n",
    "        # Initialize lists to store header and data\n",
    "        results = {}\n",
    "        football_club = {}\n",
    "        data_2 = []\n",
    "\n",
    "        # Select all rows in the table\n",
    "        table_rows = response.xpath('//tr')\n",
    "        data_rows = response.xpath(\"//tr/td\")\n",
    "        \n",
    "        for row_table, row_data in zip(table_rows, data_rows):\n",
    "            club = row_table.xpath('//a[@class = \"AnchorLink\"]/text()').getall()\n",
    "            l = row_data.xpath(\"//span[@class = 'stat-cell']\")\n",
    "        \n",
    "        club = club[:-8]\n",
    "        data = [i.xpath(\"text()\").get() for i in l]\n",
    "        #print(len(data))\n",
    "        \n",
    "        # Group the values into chunks of 8\n",
    "        grouped_values = [data[i:i+8] for i in range(0, len(data), 8)]    \n",
    "        #print(len(grouped_values))\n",
    "        #print(len(club))\n",
    "            \n",
    "        # Ensure that both the club name and at least 8 stats are available\n",
    "        for i, j in zip(club, grouped_values):\n",
    "            results[i] = {\n",
    "                'Games Played' :     j[0], \n",
    "                'Win' :              j[1],  \n",
    "                'Draw' :             j[2], \n",
    "                'Lose' :             j[3], \n",
    "                'Goals For' :        j[4],  \n",
    "                'Goals Against' :    j[5],  \n",
    "                'Goals Difference' : j[6], \n",
    "                'Points' :           j[7],  \n",
    "            }\n",
    "        \n",
    "\n",
    "        \n",
    "        # Save the results to a CSV file\n",
    "        csv_file = \"champions_league_table.csv\"\n",
    "        with open(csv_file, mode='w', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=['Club', 'Games Played', 'Win', \n",
    "                                                      'Draw', 'Lose', 'Goals For', 'Goals Against', \n",
    "                                                      'Goals Difference', 'Points'])\n",
    "            writer.writeheader()\n",
    "            \n",
    "            # Write each club's data into the CSV\n",
    "            for club_name, stats in results.items():\n",
    "                row = {'Club': club_name}\n",
    "                row.update(stats)\n",
    "                writer.writerow(row)\n",
    "\n",
    "\n",
    "        \n",
    "        # Yield the results for Scrapy's pipeline or further use\n",
    "        yield results\n",
    "                \n",
    "                \n",
    "def run_spider():\n",
    "    process = CrawlerProcess(settings=get_project_settings())\n",
    "    process.crawl(ChampionsLeagueScrapeSpider)\n",
    "    process.start()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configure logging\n",
    "    logging.getLogger('scrapy').setLevel(logging.WARNING)  \n",
    "    run_spider()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

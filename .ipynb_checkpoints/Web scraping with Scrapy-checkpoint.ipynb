{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11ff19d8-9b09-437f-8213-a6e75cb54cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from itemloaders.processors import MapCompose, TakeFirst\n",
    "from scrapy.loader import ItemLoader\n",
    "import pandas as pd\n",
    "from openpyxl import Workbook, load_workbook\n",
    "import os\n",
    "import re\n",
    "import logging"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f56e3d24",
   "metadata": {},
   "source": [
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "\n",
    "    #Website spider sends requests to\n",
    "    start_urls = ['https://books.toscrape.com/']\n",
    "    \n",
    "    # This will help to save the data generated in CSV format\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'books.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('[ OUR RESPONSE ]')\n",
    "\n",
    "        # Use the CSS selector to get the appropriate tag from the website it is in h3 and a\n",
    "        #print(response.css(\"h3 a::text\").get())\n",
    "        #print(response.css(\"article\"))\n",
    "        \n",
    "        ebooks_css = response.css(\"article\") # Using CSS selector\n",
    "        ebooks_xpath = response.xpath('//article') # Using x_path selector\n",
    "        \n",
    "        \n",
    "        # Zip the different selectors and extract the information from the for loop\n",
    "        for ebook_c, ebook_x in zip(ebooks_css, ebooks_xpath):\n",
    "            \n",
    "            \n",
    "            title_c = ebook_c.css(\"a::text\").get()\n",
    "            title_x = ebook_x.xpath('.//a[@title]/text()').get()\n",
    "            \n",
    "            \n",
    "            price_c = ebook_c.css(\"p.price_color::text\").get()\n",
    "            price_c = float(price_c[1:])\n",
    "            \n",
    "            price_x = ebook_x.xpath('.//p[@class = \"price_color\"]/text()').get()\n",
    "            price_x = float(price_x[1:])\n",
    "            \n",
    "            \n",
    "            link_c = ebook_c.css(\"a::attr(href)\").get()\n",
    "            link_x = ebook_x.xpath('.//a[@href]/text()').get()\n",
    "            \n",
    "            yield {\n",
    "                'Link_c':link_c,\n",
    "                'Link_x':link_x,\n",
    "                'Title_c': title_c,\n",
    "                \"Price_c\": price_c,\n",
    "                'Title_x': title_x,\n",
    "                \"Price_x\": price_x\n",
    "                \n",
    "            } \n",
    "            \n",
    "#@wait_for(10)\n",
    "def run_spider():\n",
    "    process = CrawlerProcess(settings=get_project_settings())\n",
    "    process.crawl(BooksToScrapeSpider)\n",
    "    process.start()\n",
    "\n",
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e37995",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# SCRAPY ITEMS \n",
    "Using Scrapy Item as shown below. We can get the data in a class BookItem\n",
    "\n",
    "```\n",
    "class BookItem(scrapy.Item):\n",
    "    link_c = scrapy.Field()\n",
    "    link_x = scrapy.Field()\n",
    "    title_c = scrapy.Field()\n",
    "    price_c = scrapy.Field()\n",
    "    title_x = scrapy.Field()\n",
    "    price_x = scrapy.Field()\n",
    "```\n",
    "\n",
    "The above class is the used in the for loop of the BookToScrapeSpider class method parse as below\n",
    "```\n",
    "for ebook_c, ebook_x in zip(ebooks_css, ebooks_xpath):\n",
    "            book_item = BookItem() # call the book item class \n",
    "            \n",
    "            book_item['title_c'] = ebook_c.css(\"a::text\").get()\n",
    "            book_item['title_x'] = ebook_x.xpath('.//a[@title]/text()').get()\n",
    "            \n",
    "            \n",
    "            price_c = ebook_c.css(\"p.price_color::text\").get()\n",
    "            book_item['price_c'] = float(price_c[1:])\n",
    "            \n",
    "            price_x = ebook_x.xpath('.//p[@class = \"price_color\"]/text()').get()\n",
    "            book_item['price_x'] = float(price_x[1:])\n",
    "            \n",
    "            \n",
    "            book_item['link_c'] = ebook_c.css(\"a::attr(href)\").get()\n",
    "            book_item['link_x'] = ebook_x.xpath('.//a[@href]/text()').get()\n",
    "            \n",
    "            yield book_item\n",
    "```\n",
    "\n",
    "I have changed the below to a raw NBconvert\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "656034f3",
   "metadata": {},
   "source": [
    "# This Class item is for the collection of data extracted from the class BookToScrapeSpider\n",
    "class BookItem(scrapy.Item):\n",
    "    link_c = scrapy.Field()\n",
    "    link_x = scrapy.Field()\n",
    "    title_c = scrapy.Field()\n",
    "    price_c = scrapy.Field()\n",
    "    title_x = scrapy.Field()\n",
    "    price_x = scrapy.Field()\n",
    "    \n",
    "    \n",
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "\n",
    "    #Website spider sends requests to\n",
    "    start_urls = ['https://books.toscrape.com/']\n",
    "    \n",
    "    # This will help to save the data generated in CSV format\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'books_item.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('[ OUR RESPONSE ]')\n",
    "\n",
    "        # Use the CSS selector to get the appropriate tag from the website it is in h3 and a\n",
    "        #print(response.css(\"h3 a::text\").get())\n",
    "        #print(response.css(\"article\"))\n",
    "        \n",
    "        ebooks_css = response.css(\"article\") # Using CSS selector\n",
    "        ebooks_xpath = response.xpath('//article') # Using x_path selector\n",
    "        \n",
    "        \n",
    "        # Zip the different selectors and extract the information from the for loop\n",
    "        for ebook_c, ebook_x in zip(ebooks_css, ebooks_xpath):\n",
    "            book_item = BookItem() # call the book item class \n",
    "            \n",
    "            book_item['title_c'] = ebook_c.css(\"a::text\").get()\n",
    "            book_item['title_x'] = ebook_x.xpath('.//a[@title]/text()').get()\n",
    "            \n",
    "            \n",
    "            price_c = ebook_c.css(\"p.price_color::text\").get()\n",
    "            book_item['price_c'] = float(price_c[1:])\n",
    "            \n",
    "            price_x = ebook_x.xpath('.//p[@class = \"price_color\"]/text()').get()\n",
    "            book_item['price_x'] = float(price_x[1:])\n",
    "            \n",
    "            \n",
    "            book_item['link_c'] = ebook_c.css(\"a::attr(href)\").get()\n",
    "            book_item['link_x'] = ebook_x.xpath('.//a[@href]/text()').get()\n",
    "            \n",
    "            yield book_item\n",
    "            \n",
    "#@wait_for(10)\n",
    "def run_spider():\n",
    "    process = CrawlerProcess(settings=get_project_settings())\n",
    "    process.crawl(BooksToScrapeSpider)\n",
    "    process.start()\n",
    "\n",
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7780b2",
   "metadata": {},
   "source": [
    "---\n",
    "# SCRAPY ITEM\n",
    "The price has a pound sign like £10.34 and I define a function te remove it like below.\n",
    "Also, i can  round the results then the Mapcompose and the TakeFirst will be very helpful\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# This Class item is for the collection of data extracted from the class BookToScrapeSpider\n",
    "class BookItem(scrapy.Item):\n",
    "    # I am able to remove the pound sign from the price_x and price_c but rounded only the price_c inside the MapCompose\n",
    "    \n",
    "    def get_price(txt):\n",
    "        return float(txt.replace('£', ''))\n",
    "\n",
    "    def round_get_price(x):\n",
    "        return round(x)\n",
    "        \n",
    "    # use the field method from scrapy to get the data directly from the parse method  \n",
    "    link_c = scrapy.Field()\n",
    "    link_x = scrapy.Field()\n",
    "    title_c = scrapy.Field()\n",
    "    price_c = scrapy.Field(input_processor = MapCompose(get_price, round_get_price),\n",
    "                                output_processor = TakeFirst())\n",
    "    title_x = scrapy.Field()\n",
    "    price_x = scrapy.Field(input_processor = MapCompose(get_price),\n",
    "                                output_processor = TakeFirst())\n",
    "\n",
    "```\n",
    "\n",
    "Then with the help of ItemLoader, I was able to use the BookItem() class.\n",
    "\n",
    "Also, i was abble to use the loader.add_css and loader.add_xpath to get the required data passing the names instead of using a dictionary directly\n",
    "\n",
    "\n",
    "```\n",
    "for ebook_c, ebook_x in zip(ebooks_css, ebooks_xpath):\n",
    "            loader_c = ItemLoader(item=BookItem(), selector=ebook_c) # Initialize the Itemloader with the BookItem Class\n",
    "            loader_c.add_css('title_c', 'a::text')\n",
    "            loader_c.add_css('price_c', 'p.price_color::text')\n",
    "            loader_c.add_css('link_c', 'a::attr(href)')\n",
    "            \n",
    "            # Create an ItemLoader for XPath context\n",
    "            loader_x = ItemLoader(item=BookItem(), selector=ebook_x) # Initialize the Itemloader with the BookItem Class\n",
    "            loader_x.add_xpath('title_x', './/a[@title]/text()')\n",
    "            loader_x.add_xpath('price_x', './/p[@class=\"price_color\"]/text()')\n",
    "            loader_x.add_xpath('link_x', './/a[@href]/text()')\n",
    "            \n",
    "            # Merge items from both loaders (you could also choose to process separately)\n",
    "            item = loader_c.load_item()\n",
    "            item.update(loader_x.load_item())\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "88fd5bfe",
   "metadata": {},
   "source": [
    "# This Class item is for the collection of data extracted from the class BookToScrapeSpider\n",
    "class BookItem(scrapy.Item):\n",
    "    \n",
    "    def get_price(txt):\n",
    "        return float(txt.replace('£', ''))\n",
    "\n",
    "    def round_get_price(x):\n",
    "        return round(x)\n",
    "    \n",
    "    \n",
    "    link_c = scrapy.Field()\n",
    "    link_x = scrapy.Field()\n",
    "    title_c = scrapy.Field()\n",
    "    price_c = scrapy.Field(input_processor = MapCompose(get_price, round_get_price),\n",
    "                                output_processor = TakeFirst())\n",
    "    title_x = scrapy.Field()\n",
    "    price_x = scrapy.Field(input_processor = MapCompose(get_price),\n",
    "                                output_processor = TakeFirst())\n",
    "\n",
    "\n",
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "\n",
    "    #Website spider sends requests to\n",
    "    start_urls = ['https://books.toscrape.com/']\n",
    "    \n",
    "\n",
    "    # This will help to save the data generated in CSV format\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'books_item_loader.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('[ OUR RESPONSE ]')\n",
    "\n",
    "        \n",
    "        ebooks_css = response.css(\"article\") # Using CSS selector\n",
    "        ebooks_xpath = response.xpath('//article') # Using x_path selector\n",
    "        \n",
    "        \n",
    "        # Zip the different selectors and extract the information from the for loop\n",
    "        for ebook_c, ebook_x in zip(ebooks_css, ebooks_xpath):\n",
    "            loader_c = ItemLoader(item=BookItem(), selector=ebook_c) # Initialize the Itemloader with the BookItem Class\n",
    "            loader_c.add_css('title_c', 'a::text')\n",
    "            loader_c.add_css('price_c', 'p.price_color::text')\n",
    "            loader_c.add_css('link_c', 'a::attr(href)')\n",
    "            \n",
    "            # Create an ItemLoader for XPath context\n",
    "            loader_x = ItemLoader(item=BookItem(), selector=ebook_x) # Initialize the Itemloader with the BookItem Class\n",
    "            loader_x.add_xpath('title_x', './/a[@title]/text()')\n",
    "            loader_x.add_xpath('price_x', './/p[@class=\"price_color\"]/text()')\n",
    "            loader_x.add_xpath('link_x', './/a[@href]/text()')\n",
    "            \n",
    "            # Merge items from both loaders (you could also choose to process separately)\n",
    "            item = loader_c.load_item()\n",
    "            item.update(loader_x.load_item())\n",
    "            \n",
    "            yield item\n",
    "            \n",
    "\n",
    "def run_spider():\n",
    "    process = CrawlerProcess(settings=get_project_settings())\n",
    "    process.crawl(BooksToScrapeSpider)\n",
    "    process.start()\n",
    "\n",
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee46ae4e",
   "metadata": {},
   "source": [
    "# SCRAPY PIPELINE\n",
    "\n",
    "The yield statement from our parse method in the BooksToScrapeSpider is passed to the pipline.\n",
    "\n",
    "Install !pip install openpyxl\n",
    "\n",
    "The spider in the method helps to access name and start url in the BooksToScrapeSpider class\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a74b3bf4",
   "metadata": {},
   "source": [
    "# Helper functions for price processing\n",
    "def get_price(txt):\n",
    "    return float(txt.replace('£', ''))\n",
    "\n",
    "def round_get_price(x):\n",
    "    return round(x)\n",
    "\n",
    "# Define the item class\n",
    "class BookItem(scrapy.Item):\n",
    "    link_c = scrapy.Field(output_processor=TakeFirst())\n",
    "    link_x = scrapy.Field(output_processor=TakeFirst())\n",
    "    title_c = scrapy.Field(output_processor=TakeFirst())\n",
    "    price_c = scrapy.Field(input_processor=MapCompose(get_price, round_get_price),\n",
    "                           output_processor=TakeFirst())\n",
    "    title_x = scrapy.Field(output_processor=TakeFirst())\n",
    "    price_x = scrapy.Field(input_processor=MapCompose(get_price),\n",
    "                           output_processor=TakeFirst())\n",
    "\n",
    "# Define the pipeline class\n",
    "class BookScraperPipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        # Check if the file exists\n",
    "        if os.path.exists('Books_pipeline.xlsx'):\n",
    "            # Load the existing workbook\n",
    "            self.workbook = load_workbook('Books_pipeline.xlsx')\n",
    "            self.sheet = self.workbook.active\n",
    "        else:\n",
    "            # Create a new workbook if it doesn't exist\n",
    "            self.workbook = Workbook()\n",
    "            self.sheet = self.workbook.active\n",
    "            self.sheet.title = 'books'\n",
    "            self.sheet.append(spider.cols)  # Add header row only if file doesn't exist\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # Append the scraped item data, ensuring no lists are passed to the Excel file\n",
    "        self.sheet.append([\n",
    "            item.get('link_c', ''),\n",
    "            item.get('link_x', ''),\n",
    "            item.get('price_c', ''),\n",
    "            item.get('price_x', ''),\n",
    "            item.get('title_c', ''),\n",
    "            item.get('title_x', '')\n",
    "        ])\n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        # Save the workbook when the spider is done\n",
    "        self.workbook.save('Books_pipeline.xlsx')\n",
    "\n",
    "# Spider class for scraping book data\n",
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "    start_urls = ['https://books.toscrape.com/']\n",
    "\n",
    "    cols = ['link_c', 'link_x', 'price_c', 'price_x', 'title_c', 'title_x']\n",
    "\n",
    "    def parse(self, response):\n",
    "        # Get ebooks using both CSS and XPath\n",
    "        ebooks_css = response.css(\"article\")\n",
    "        ebooks_xpath = response.xpath('//article')\n",
    "        \n",
    "        # Iterate through the zipped CSS and XPath selections\n",
    "        for ebook_c, ebook_x in zip(ebooks_css, ebooks_xpath):\n",
    "            loader_c = ItemLoader(item=BookItem(), selector=ebook_c)  # Loader for CSS\n",
    "            loader_c.add_css('title_c', 'a::text')\n",
    "            loader_c.add_css('price_c', 'p.price_color::text')\n",
    "            loader_c.add_css('link_c', 'a::attr(href)')\n",
    "            \n",
    "            loader_x = ItemLoader(item=BookItem(), selector=ebook_x)  # Loader for XPath\n",
    "            loader_x.add_xpath('title_x', './/a[@title]/text()')\n",
    "            loader_x.add_xpath('price_x', './/p[@class=\"price_color\"]/text()')\n",
    "            loader_x.add_xpath('link_x', './/a[@href]/text()')\n",
    "            \n",
    "            # Load items from both CSS and XPath loaders and merge\n",
    "            item = loader_c.load_item()\n",
    "            item.update(loader_x.load_item())\n",
    "            \n",
    "            yield item\n",
    "\n",
    "\n",
    "process = CrawlerProcess(settings={\n",
    "    'LOG_LEVEL': 'ERROR',  # Reduce log clutter\n",
    "    'ITEM_PIPELINES': {\n",
    "        '__main__.BookScraperPipeline': 1, \n",
    "    }\n",
    "})\n",
    "\n",
    "# Run the spider\n",
    "process.crawl(BooksToScrapeSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52df790",
   "metadata": {},
   "source": [
    "---\n",
    "# PAGINATION\n",
    "\n",
    "Pagination helps us to get the next page of the website.\n",
    "\n",
    "```\n",
    " def __init__(self):\n",
    "        super().__init__()\n",
    "        self.page_count = 1\n",
    "        self.total_pages = 4\n",
    "        \n",
    "    def start_requests(self):\n",
    "        base_url = 'https://books.toscrape.com/catalogue/category/books/sequential-art_5'\n",
    "        \n",
    "        while self.page_count <= self.total_pages:\n",
    "            print(f'Page Count : {self.page_count}')\n",
    "            yield scrapy.Request(f\"{base_url}/page-{self.page_count}.html\")\n",
    "                        \n",
    "            self.page_count += 1\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bbd7d325",
   "metadata": {},
   "source": [
    "# Define the item class\n",
    "class BookItem(scrapy.Item):\n",
    "    \n",
    "    # Helper functions for price processing\n",
    "    def get_price(txt):\n",
    "        return float(txt.replace('£', ''))\n",
    "    \n",
    "    def round_get_price(x):\n",
    "        return round(x)\n",
    "    \n",
    "    # get items from using the the Field method\n",
    "    link_c = scrapy.Field(output_processor=TakeFirst())\n",
    "    link_x = scrapy.Field(output_processor=TakeFirst())\n",
    "    title_c = scrapy.Field(output_processor=TakeFirst())\n",
    "    price_c = scrapy.Field(input_processor=MapCompose(get_price, round_get_price),\n",
    "                           output_processor=TakeFirst())\n",
    "    title_x = scrapy.Field(output_processor=TakeFirst())\n",
    "    price_x = scrapy.Field(input_processor=MapCompose(get_price),\n",
    "                           output_processor=TakeFirst())\n",
    "\n",
    "# Define the pipeline class\n",
    "class BookScraperPipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        # Check if the file exists\n",
    "        if os.path.exists('Books_pagination.xlsx'):\n",
    "            # Load the existing workbook\n",
    "            self.workbook = load_workbook('Books_pagination.xlsx')\n",
    "            self.sheet = self.workbook.active\n",
    "        else:\n",
    "            # Create a new workbook if it doesn't exist\n",
    "            self.workbook = Workbook()\n",
    "            self.sheet = self.workbook.active\n",
    "            self.sheet.title = 'books'\n",
    "            self.sheet.append(spider.cols)  # Add header row only if file doesn't exist\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # Append the scraped item data\n",
    "        self.sheet.append([\n",
    "            item.get('link_c', ''),\n",
    "            item.get('link_x', ''),\n",
    "            item.get('price_c', ''),\n",
    "            item.get('price_x', ''),\n",
    "            item.get('title_c', ''),\n",
    "            item.get('title_x', '')\n",
    "        ])\n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        # Save the workbook when the spider is done\n",
    "        self.workbook.save('Books_pagination.xlsx')\n",
    "\n",
    "# Spider class for scraping book data\n",
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "    start_urls = ['https://books.toscrape.com/catalogue/category/books/mystery_3/']\n",
    "\n",
    "    cols = ['link_c', 'link_x', 'price_c', 'price_x', 'title_c', 'title_x']\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.page_count = 0\n",
    "\n",
    "    def parse(self, response):\n",
    "        self.page_count += 1\n",
    "        # Get ebooks using both CSS and XPath\n",
    "        ebooks_css = response.css(\"article\")\n",
    "        ebooks_xpath = response.xpath('//article')\n",
    "        \n",
    "        # Iterate through the zipped CSS and XPath selections\n",
    "        for ebook_c, ebook_x in zip(ebooks_css, ebooks_xpath):\n",
    "            loader_c = ItemLoader(item=BookItem(), selector=ebook_c)  # Loader for CSS\n",
    "            loader_c.add_css('title_c', 'a::text')\n",
    "            loader_c.add_css('price_c', 'p.price_color::text')\n",
    "            loader_c.add_css('link_c', 'a::attr(href)')\n",
    "            \n",
    "            loader_x = ItemLoader(item=BookItem(), selector=ebook_x)  # Loader for XPath\n",
    "            loader_x.add_xpath('title_x', './/a[@title]/text()')\n",
    "            loader_x.add_xpath('price_x', './/p[@class=\"price_color\"]/text()')\n",
    "            loader_x.add_xpath('link_x', './/a[@href]/text()')\n",
    "            \n",
    "            # Load items from both CSS and XPath loaders and merge\n",
    "            item = loader_c.load_item()\n",
    "            item.update(loader_x.load_item())\n",
    "            \n",
    "            yield item\n",
    "        \n",
    "        print(f'Page Count : {self.page_count}')\n",
    "        \n",
    "        # Handle pagination - for the next page button\n",
    "        next_btn = response.css('li.next a::attr(href)').get()\n",
    "        \n",
    "        # This will help to stop the loop once the last page is reached.\n",
    "        if next_btn:\n",
    "            next_page = response.urljoin(next_btn)  # This is the full url\n",
    "            yield scrapy.Request(url=next_page, callback=self.parse)\n",
    "\n",
    "# Set up and run the crawler process\n",
    "process = CrawlerProcess(settings={\n",
    "    'LOG_LEVEL': 'ERROR',  # Reduce log clutter\n",
    "    'ITEM_PIPELINES': {\n",
    "        '__main__.BookScraperPipeline': 1, \n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(BooksToScrapeSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5fa646f9",
   "metadata": {},
   "source": [
    "# Define the item class to collect all the columns needed\n",
    "class BookItem(scrapy.Item):\n",
    "    \n",
    "    # Helper functions for price processing\n",
    "    def get_price(txt):\n",
    "        return float(txt.replace('£', ''))\n",
    "    \n",
    "    def round_get_price(x):\n",
    "        return round(x)\n",
    "    \n",
    "    # get items from using the the Field method\n",
    "    link_c = scrapy.Field(output_processor=TakeFirst())\n",
    "    link_x = scrapy.Field(output_processor=TakeFirst())\n",
    "    title_c = scrapy.Field(output_processor=TakeFirst())\n",
    "    price_c = scrapy.Field(input_processor=MapCompose(get_price, round_get_price),\n",
    "                           output_processor=TakeFirst())\n",
    "    title_x = scrapy.Field(output_processor=TakeFirst())\n",
    "    price_x = scrapy.Field(input_processor=MapCompose(get_price),\n",
    "                           output_processor=TakeFirst())\n",
    "\n",
    "# Define the pipeline class\n",
    "class BookScraperPipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        # Check if the file exists # Change this to where you want it saved\n",
    "        if os.path.exists('Books_pag_requests.xlsx'):\n",
    "            # Load the existing workbook # Change this to where you want it saved\n",
    "            self.workbook = load_workbook('Books_pag_requests.xlsx')\n",
    "            self.sheet = self.workbook.active\n",
    "        else:\n",
    "            # Create a new workbook if it doesn't exist\n",
    "            self.workbook = Workbook()\n",
    "            self.sheet = self.workbook.active\n",
    "            self.sheet.title = 'books'\n",
    "            self.sheet.append(spider.cols)  # Add header row only if file doesn't exist\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # Append the scraped item data\n",
    "        self.sheet.append([\n",
    "            item.get('link_c', ''),\n",
    "            item.get('link_x', ''),\n",
    "            item.get('price_c', ''),\n",
    "            item.get('price_x', ''),\n",
    "            item.get('title_c', ''),\n",
    "            item.get('title_x', '')\n",
    "        ])\n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        # Save the workbook when the spider is done\n",
    "        self.workbook.save('Books_pag_requests.xlsx') # Change this to where you want it saved\n",
    "\n",
    "# Spider class for scraping book data\n",
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "    #start_urls = ['https://books.toscrape.com/catalogue/category/books/mystery_3/']\n",
    "\n",
    "    cols = ['link_c', 'link_x', 'price_c', 'price_x', 'title_c', 'title_x']\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.page_count = 1\n",
    "        self.total_pages = 4 # Total number of pages to scrape\n",
    "        \n",
    "    def start_requests(self):\n",
    "        base_url = 'https://books.toscrape.com/catalogue/category/books/sequential-art_5' # The start url \n",
    "        \n",
    "        while self.page_count <= self.total_pages:\n",
    "            print(f'Page Count : {self.page_count}')\n",
    "            yield scrapy.Request(f\"{base_url}/page-{self.page_count}.html\")\n",
    "                        \n",
    "            self.page_count +=  1             \n",
    "                      \n",
    "    def parse(self, response):\n",
    "        self.page_count += 1\n",
    "        # Get ebooks using both CSS and XPath\n",
    "        ebooks_css = response.css(\"article\")\n",
    "        ebooks_xpath = response.xpath('//article')\n",
    "        \n",
    "        # Iterate through the zipped CSS and XPath selections\n",
    "        for ebook_c, ebook_x in zip(ebooks_css, ebooks_xpath):\n",
    "            #https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpaths\n",
    "            loader_c = ItemLoader(item=BookItem(), selector=ebook_c)  # Loader for CSS \n",
    "            loader_c.add_css('title_c', 'a::text')\n",
    "            loader_c.add_css('price_c', 'p.price_color::text')\n",
    "            loader_c.add_css('link_c', 'a::attr(href)')\n",
    "            \n",
    "            #https://docs.scrapy.org/en/latest/topics/selectors.html#working-with-xpaths\n",
    "            loader_x = ItemLoader(item=BookItem(), selector=ebook_x)  # Loader for XPath\n",
    "            loader_x.add_xpath('title_x', './/a[@title]/text()')\n",
    "            loader_x.add_xpath('price_x', './/p[@class=\"price_color\"]/text()')\n",
    "            loader_x.add_xpath('link_x', './/a/@href')\n",
    "            \n",
    "            \n",
    "            # Load items from both CSS and XPath loaders and merge\n",
    "            item = loader_c.load_item()\n",
    "            item.update(loader_x.load_item())\n",
    "            \n",
    "            yield item\n",
    "        \n",
    "#         print(f'Page Count : {self.page_count}')\n",
    "        \n",
    "#         # Handle pagination - for the next page button\n",
    "#         next_btn = response.css('li.next a::attr(href)').get()\n",
    "        \n",
    "#         # This will help to stop the loop once the last page is reached.\n",
    "#         if next_btn:\n",
    "#             next_page = response.urljoin(next_btn)  # This is the full url\n",
    "#             yield scrapy.Request(url=next_page, callback=self.parse)\n",
    "\n",
    "# Set up and run the crawler process\n",
    "process = CrawlerProcess(settings={\n",
    "    'LOG_LEVEL': 'ERROR',  # Reduce log clutter\n",
    "    'ITEM_PIPELINES': {\n",
    "        '__main__.BookScraperPipeline': 1, \n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(BooksToScrapeSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70869d",
   "metadata": {},
   "source": [
    "# FOLLOWING THE LINK\n",
    "\n",
    "## CSS SELECTOR METHOD"
   ]
  },
  {
   "cell_type": "raw",
   "id": "41b6da69",
   "metadata": {},
   "source": [
    "# Define the item class to collect all the columns needed\n",
    "class BookItem(scrapy.Item):\n",
    "    \n",
    "    # Helper functions for price processing\n",
    "    def get_price(txt):\n",
    "        return float(txt.replace('£', ''))\n",
    "    \n",
    "    def get_quantity(x):\n",
    "        x = x.replace('()', '')\n",
    "        x = x.split()\n",
    "        txt = x[0]\n",
    "        return int(txt)\n",
    "    \n",
    "    # CSS METHOD\n",
    "    title_c = scrapy.Field(output_processor=TakeFirst())\n",
    "    price_c = scrapy.Field(input_processor=MapCompose(get_price), output_processor=TakeFirst())\n",
    "    quantity_c = scrapy.Field(input_processor=MapCompose(get_quantity), output_processor=TakeFirst())\n",
    "\n",
    "\n",
    "# Define the pipeline class\n",
    "class BookScraperPipeline:\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        # Check if the file exists\n",
    "        if os.path.exists('Books_links.xlsx'):\n",
    "            # Load the existing workbook\n",
    "            self.workbook = load_workbook('Books_links.xlsx')\n",
    "            self.sheet = self.workbook.active\n",
    "        else:\n",
    "            # Create a new workbook if it doesn't exist\n",
    "            self.workbook = Workbook()\n",
    "            self.sheet = self.workbook.active\n",
    "            self.sheet.title = 'books'\n",
    "            self.sheet.append(spider.cols)  # Add header row only if file doesn't exist\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # Append the scraped item data\n",
    "        self.sheet.append([\n",
    "            item.get('price_c', ''),\n",
    "            item.get('title_c', ''),\n",
    "            item.get('quantity_c', ''),\n",
    "        ])\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        # Save the workbook when the spider is done\n",
    "        self.workbook.save('Books_links.xlsx')\n",
    "\n",
    "\n",
    "# Spider class for scraping book data\n",
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "    start_urls = ['https://books.toscrape.com/catalogue/category/books/sequential-art_5/']\n",
    "    cols = ['price_c', 'title_c', 'quantity_c']\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.page_count = 1\n",
    "        self.total_pages = 4  # Total number of pages to scrape\n",
    "        \n",
    "    def start_requests(self):\n",
    "        base_url = 'https://books.toscrape.com/catalogue/category/books/sequential-art_5' \n",
    "        \n",
    "        while self.page_count <= self.total_pages:\n",
    "            print(f'PAGE: {self.page_count}')\n",
    "            yield scrapy.Request(f\"{base_url}/page-{self.page_count}.html\")\n",
    "            self.page_count += 1  \n",
    "                      \n",
    "    def parse(self, response):\n",
    "        # Get ebooks using CSS selector\n",
    "        ebooks_css = response.css(\"article.product_pod\")\n",
    "        \n",
    "        for ebook_c in ebooks_css:\n",
    "            # Extract the relative URL and join with base URL\n",
    "            relative_url_c = ebook_c.css('h3 a::attr(href)').get()\n",
    "            full_url_c = response.urljoin(relative_url_c)\n",
    "            \n",
    "            yield scrapy.Request(url=full_url_c, callback=self.parse_details_css)\n",
    "            \n",
    "    def parse_details_css(self, response):\n",
    "        main = response.css('div.product_main')\n",
    "        loader_c = ItemLoader(item=BookItem(), selector=main)\n",
    "        \n",
    "        # Extract title, price, and quantity\n",
    "        loader_c.add_css('title_c', 'h1::text')\n",
    "        loader_c.add_css('price_c', 'p.price_color::text')\n",
    "        quantity_p = main.css(\"p.availability\")\n",
    "        loader_c.add_value(\"quantity_c\", quantity_p.re(r'\\((\\d+) available\\)')[0])\n",
    "        \n",
    "        yield loader_c.load_item()\n",
    "\n",
    "\n",
    "# Set up and run the crawler process\n",
    "process = CrawlerProcess(settings={\n",
    "    'LOG_LEVEL': 'ERROR',  # Reduce log clutter\n",
    "    'ITEM_PIPELINES': {\n",
    "        '__main__.BookScraperPipeline': 1, \n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(BooksToScrapeSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1035ffa",
   "metadata": {},
   "source": [
    "## Xpath method"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3566a7a3",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Define the item class to collect all the columns needed\n",
    "class BookItem(scrapy.Item):\n",
    "    # Helper functions for price processing\n",
    "    def get_price(txt):\n",
    "        return float(txt.replace('£', ''))\n",
    "    \n",
    "    def get_quantity(x):\n",
    "        x = x.replace('()', '')\n",
    "        x = x.split()\n",
    "        txt = x[0]\n",
    "        return int(txt)\n",
    "    \n",
    "    # XPath METHOD\n",
    "    title_x = scrapy.Field(output_processor=TakeFirst())\n",
    "    price_x = scrapy.Field(input_processor=MapCompose(get_price), output_processor=TakeFirst())\n",
    "    quantity_x = scrapy.Field(input_processor=MapCompose(get_quantity), output_processor=TakeFirst())\n",
    "\n",
    "# Define the pipeline class\n",
    "class BookScraperPipeline:\n",
    "    def open_spider(self, spider):\n",
    "        # Check if the file exists\n",
    "        if os.path.exists('Books_links_x.xlsx'):\n",
    "            # Load the existing workbook\n",
    "            self.workbook = load_workbook('Books_links_x.xlsx')\n",
    "            self.sheet = self.workbook.active\n",
    "        else:\n",
    "            # Create a new workbook if it doesn't exist\n",
    "            self.workbook = Workbook()\n",
    "            self.sheet = self.workbook.active\n",
    "            self.sheet.title = 'books'\n",
    "            self.sheet.append(spider.cols)  # Add header row only if file doesn't exist\n",
    "\n",
    "    def process_item(self, item, spider):\n",
    "        # Append the scraped item data\n",
    "        self.sheet.append([\n",
    "            item.get('price_x', ''),\n",
    "            item.get('title_x', ''),\n",
    "            item.get('quantity_x', ''),\n",
    "        ])\n",
    "        return item\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        # Save the workbook when the spider is done\n",
    "        self.workbook.save('Books_links_x.xlsx')\n",
    "\n",
    "# Spider class for scraping book data\n",
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "    start_urls = ['https://books.toscrape.com/catalogue/category/books/sequential-art_5/']\n",
    "    cols = ['price_x', 'title_x', 'quantity_x']\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.page_count = 1\n",
    "        self.total_pages = 4  # Total number of pages to scrape\n",
    "        \n",
    "    def start_requests(self):\n",
    "        base_url = 'https://books.toscrape.com/catalogue/category/books/sequential-art_5' \n",
    "        \n",
    "        while self.page_count <= self.total_pages:\n",
    "            print(f'PAGE: {self.page_count}')\n",
    "            yield scrapy.Request(f\"{base_url}/page-{self.page_count}.html\")\n",
    "            self.page_count += 1  \n",
    "                      \n",
    "    def parse(self, response):\n",
    "        # Get ebooks using Xpath selector\n",
    "        ebooks_xpath = response.xpath(\"//article[@class='product_pod']\")\n",
    "        \n",
    "        for ebook_x in ebooks_xpath:\n",
    "            # Extract the relative URL and join with base URL\n",
    "            relative_url_x = ebook_x.xpath('.//a/@href').get()\n",
    "            full_url_x = response.urljoin(relative_url_x)\n",
    "            \n",
    "            yield scrapy.Request(url=full_url_x, callback=self.parse_details_xpath)\n",
    "            \n",
    "    def parse_details_xpath(self, response):\n",
    "        main = response.xpath(\"//div[@class='product_main']\")\n",
    "        loader_x = ItemLoader(item=BookItem(), selector=main)\n",
    "        \n",
    "        # Extract title and price using XPath\n",
    "        title_x = response.xpath('//h1/text()').get()\n",
    "        price_x = response.xpath('//p[@class=\"price_color\"]/text()').get()\n",
    "\n",
    " \n",
    "\n",
    "        loader_x.add_value('title_x', title_x)\n",
    "        loader_x.add_value('price_x', price_x)\n",
    "\n",
    "        # Extract quantity text and apply regex\n",
    "        quantity_text = ''.join(response.xpath('//p[@class=\"instock availability\"]/text()').getall()).strip()\n",
    "        quantity_match = re.search(r'\\((\\d+) available\\)', quantity_text)\n",
    "        quantity = quantity_match.group(1) if quantity_match else 'Quantity not found'\n",
    "        loader_x.add_value('quantity_x', quantity)\n",
    "\n",
    "        yield loader_x.load_item()\n",
    "\n",
    "# Set up and run the crawler process\n",
    "process = CrawlerProcess(settings={\n",
    "    'LOG_LEVEL': 'ERROR',  # Reduce log clutter\n",
    "    'ITEM_PIPELINES': {\n",
    "        '__main__.BookScraperPipeline': 1, \n",
    "    }\n",
    "})\n",
    "\n",
    "process.crawl(BooksToScrapeSpider)\n",
    "process.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e68c06",
   "metadata": {},
   "source": [
    "# EXTRACTING TABLES"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d74263a6",
   "metadata": {},
   "source": [
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "\n",
    "    #Website spider sends requests to\n",
    "    start_urls = ['https://books.toscrape.com/catalogue/the-mysterious-affair-at-styles-hercule-poirot-1_452/index.html']\n",
    "    \n",
    "    # This will help to save the data generated in CSV format\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'books_table.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('[ OUR RESPONSE ]')\n",
    "        \n",
    "        table =  response.css('table')\n",
    "        \n",
    "        \n",
    "        product_details = {}\n",
    "        \n",
    "        for row in table.css('tr'):\n",
    "            heading = row.css(\"th::text\").get()\n",
    "            data = row.css(\"td::text\").get()\n",
    "            \n",
    "            product_details[heading] = data\n",
    "\n",
    "            \n",
    "        yield product_details\n",
    "            \n",
    "#@wait_for(10)\n",
    "def run_spider():\n",
    "    process = CrawlerProcess(settings=get_project_settings())\n",
    "    process.crawl(BooksToScrapeSpider)\n",
    "    process.start()\n",
    "\n",
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0457bb",
   "metadata": {},
   "source": [
    "# EXTRACTING TABLE FROM codecademy beautifulsoup course using SCRAPY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb8b1570",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-12 19:48:16 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
      "2024-09-12 19:48:16 [scrapy.utils.log] INFO: Versions: lxml 5.3.0.0, libxml2 2.11.7, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.13 | packaged by Anaconda, Inc. | (main, Sep 11 2023, 13:24:38) [MSC v.1916 64 bit (AMD64)], pyOpenSSL 24.2.1 (OpenSSL 3.3.2 3 Sep 2024), cryptography 43.0.1, Platform Windows-10-10.0.22631-SP0\n",
      "2024-09-12 19:48:16 [py.warnings] WARNING: C:\\Users\\conso\\.conda\\envs\\py310\\lib\\site-packages\\scrapy\\utils\\request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ OUR RESPONSE ]\n"
     ]
    }
   ],
   "source": [
    "class BooksToScrapeSpider(scrapy.Spider):\n",
    "    name = 'bookspider'\n",
    "\n",
    "    # Website spider sends requests to\n",
    "    start_urls = ['https://content.codecademy.com/courses/beautifulsoup/cacao/index.html']\n",
    "    \n",
    "    # This will help to save the data generated in CSV format\n",
    "    custom_settings = {\n",
    "        'FEEDS': {\n",
    "            'cacao_table.csv': {\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        },\n",
    "        'LOG_LEVEL': 'WARNING',  # Set the logging level to WARNING\n",
    "        #'LOG_FILE': 'scrapy_log.txt'  # Optional: Save log messages to a file\n",
    "    }\n",
    "\n",
    "    def parse(self, response):\n",
    "        print('[ OUR RESPONSE ]')\n",
    "        \n",
    "        # Initialize lists to store header and data\n",
    "        header = []\n",
    "        data = []\n",
    "\n",
    "        # Select all rows in the table\n",
    "        rows = response.xpath('//tr')\n",
    "\n",
    "        # Iterate over each row\n",
    "        for index, row in enumerate(rows):\n",
    "            # Select all <td> elements in the current row and get their text\n",
    "            tds = row.xpath('td')\n",
    "            row_data = [td.xpath('text()').get() for td in tds]\n",
    "\n",
    "            # Handle header row\n",
    "            if index == 2:\n",
    "                header = row_data\n",
    "                #print('Header:', header)\n",
    "\n",
    "            # Collect data rows starting from index 3 onward\n",
    "            elif index >= 3:\n",
    "                data.append(row_data)\n",
    "\n",
    "        # Combine header with data into rows\n",
    "        if header and data:\n",
    "            for row in data:\n",
    "                # Yield each row as a dictionary with header as keys\n",
    "                yield dict(zip(header, row))\n",
    "\n",
    "                \n",
    "                \n",
    "def run_spider():\n",
    "    process = CrawlerProcess(settings=get_project_settings())\n",
    "    process.crawl(BooksToScrapeSpider)\n",
    "    process.start()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Configure logging\n",
    "    logging.getLogger('scrapy').setLevel(logging.WARNING)  # Show only warnings and errors\n",
    "    run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178db67f",
   "metadata": {},
   "source": [
    "# SCRAPING JAVASCRIPT RENDERED WEBSITES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23021a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
